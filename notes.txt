
*** Suggesting queries ***
ð‘†ð‘ð‘œð‘Ÿð‘’(ð¶ð‘„, ð‘žâ€²) =>
-ð¹ð‘Ÿð‘’ð‘ž(ð¶ð‘„) =  ð¹ð‘Ÿð‘’ð‘ž(ð¶ð‘„) is the frequency of occurrence of ð¶ð‘„ in QL divided by 
the maximum frequency of occurrence of any query in QL

-ð‘€ð‘œð‘‘(ð¶ð‘„,ð‘žâ€²) is the number of sessions in which qâ€™ is modified to ð¶ð‘„ divided by 
the total number of sessions in QL in which qâ€™ appears
=>
the number of sessions in which qâ€™ is modified to ð¶ð‘„ => 
the total number of sessions in QL in which qâ€™ appears => 


-ð‘‡ð‘–ð‘šð‘’(ð¶ð‘„, ð‘žâ€²) is min difference between the time occurrence of qâ€™ and 
ð¶ð‘„ across sessions in which both qâ€™ and ð¶ð‘„ appear over length of the longest session in QL.
=>
min difference between the time occurrence of qâ€™ and ð¶ð‘„ across sessions in which both qâ€™ and ð¶ð‘„ appear 
length of the longest session in QL.





-example:
q' = 
CQ = 



Questions:
ð‘€ð‘œð‘‘(ð¶ð‘„,ð‘žâ€²) is the number of sessions in which qâ€™ is modified to ð¶ð‘„ => what would be to be modified to?
It's the probability that each q' changes to CQ.
cuantas sesiones cambiaron de 'hello' a 'hello world' en cuantas sesiones sobre el total => eso es la probabilidad de mod
q' = 'hello'
CQ1 = hello world
CQ2 = hello my
=> I have to look for all sessions which ð‘žâ€² appears and then look for the next queries of the same session (same user) to see if ð‘žâ€² has added words, if it does (like 'hello world' as an example) then add it to the dictionary with word as key and a counter as value. Then for each one calculate the probability based on the total number of sessions in QL in which qâ€™ appears



This is the min of every freq, mod, time across all data. Do we need to sum those values or how do they interact with 1?
1 âˆ’ ð‘€ð‘–ð‘›{ð¹ð‘Ÿð‘’ð‘ž(ð¶ð‘„), ð‘€ð‘œð‘‘(ð¶ð‘„, ð‘žâ€²), ð‘‡ð‘–ð‘šð‘’(ð¶ð‘„, ð‘žâ€²)} => el mÃ­nimo de esos tres valores



in Mod, the part:  the total number of sessions in QL in which qâ€™ appears. This means 

q = texas

1 texas 10:30 => query
1 texas uni 10:31 => candidate
1 texas university 10:32 => candidate
1 texas university students 10:33 => candidate


1 texas 10:30 => query
1 uni 10:31
1 texas university 10:32 => candidate
1 texas university students 10:33 => candidate
2 texas => query
2 texas ss => candidate
3 texas => query
4 texas instr


QUESTIONS:
ð‘‡ð‘–ð‘šð‘’(ð¶ð‘„, ð‘žâ€²) is min difference between the time occurrence of qâ€™ and ð¶ð‘„ across sessions in which both qâ€™ and ð¶ð‘„ appear over length of the longest session in QL.
Can we talk more about the Time calculation? How should we calculate it? what about the longest session? 



***************************************************************************

Technical Notes

To start mongo: 
cd  ~/Software/mongodb/bin
./mongod
- ./mongo 	=> mongo client
https://docs.mongodb.com/manual/reference/mongo-shell/
use <db>
To recreate collections:
db.collection_documents.drop()
db.inverted_index.drop()
db.max_freq_doc.drop()

db.createCollection("collection_documents")
db.createCollection("max_freq_doc")
db.createCollection("inverted_index")    

db.getCollection('collection_documents').createIndex({id: 1})
db.getCollection('max_freq_doc').createIndex({doc_id: 1})
db.getCollection('inverted_index').createIndex({term: 1})

db.getCollection('collection_documents').find({}).length()
db.getCollection('inverted_index').find({}).length()
db.getCollection('max_freq_doc').find({}).length()

db.getCollection('collection_documents').find({}).length() // 630000
db.getCollection('collection_documents').find({id: 620001})
db.getCollection('collection_documents').remove({id: {$gt: 620000}})

db.getCollection('max_freq_doc').find({doc_id: 620000})


To start redis:
redis-server /usr/local/etc/redis.conf
redis-cli => redis client
FLUSHALL => to delete all keys from all Redis databases
FLUSHDB => To delete all keys of the selected Redis database only
del myhash => delete my entire mynahs
HGETALL myhash => get all keys and values from myhash
	 HGETALL collection_documents 
	 HGETALL inverted_index
HLEN myhash => returns the size of elements
HGET collection_documents 1000
GET tubul

We're using python3:
python3 controller.py


Execution Logs:
Test: Save in mongo documents and index
1000 => 173.997 seconds (2.90 minutes)
for 1.6 million docs, it will take 80 hours

Test: Save index in memory only
1000 => 10 seconds in multiprocessing. 32 seconds in serie.
2000 => 16 seconds in multiprocessing. 77.31 seconds in serie.
20000 => 166.38 seconds in multiprocessing.  seconds in serie.
for 1.6 million docs, it will take 36.97 hours

Test: Save index in redis only
1000 => 24.65 seconds in multiprocessing. 
2000 => 43.63 seconds in multiprocessing.
20000 => 184.97 seconds in multiprocessing.

Test: Save in mongo the index
1000 => 145.92 seconds


ubuntu commands for mongodb:
sudo systemctl enable mongod
sudo systemctl start mongod 

sudo systemctl stop mongod
sudo systemctl restart mongod 

test on ubuntu:
start time: 2019-10-14 19:29:35.110371


******************
Steps
X = 50 000 docs
1) Loop by batch size of X
2) For every doc read, save it to mongo
3) Calculate the index structures for the batch and loop through them to save a unified index structure for the X docs to save as a pickle file.
4) Loop the folder of the pickle files and iterate them to build a unified index structure, then save that file and iterate through it to save it in mongo.




Ã±



