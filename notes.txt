
*** Suggesting queries ***
𝑆𝑐𝑜𝑟𝑒(𝐶𝑄, 𝑞′) =>
-𝐹𝑟𝑒𝑞(𝐶𝑄) =  𝐹𝑟𝑒𝑞(𝐶𝑄) is the frequency of occurrence of 𝐶𝑄 in QL divided by 
the maximum frequency of occurrence of any query in QL

-𝑀𝑜𝑑(𝐶𝑄,𝑞′) is the number of sessions in which q’ is modified to 𝐶𝑄 divided by 
the total number of sessions in QL in which q’ appears
=>
the number of sessions in which q’ is modified to 𝐶𝑄 => 
the total number of sessions in QL in which q’ appears => 


-𝑇𝑖𝑚𝑒(𝐶𝑄, 𝑞′) is min difference between the time occurrence of q’ and 
𝐶𝑄 across sessions in which both q’ and 𝐶𝑄 appear over length of the longest session in QL.
=>
min difference between the time occurrence of q’ and 𝐶𝑄 across sessions in which both q’ and 𝐶𝑄 appear 
length of the longest session in QL.





-example:
q' = 
CQ = 



Questions:
𝑀𝑜𝑑(𝐶𝑄,𝑞′) is the number of sessions in which q’ is modified to 𝐶𝑄 => what would be to be modified to?
It's the probability that each q' changes to CQ.
cuantas sesiones cambiaron de 'hello' a 'hello world' en cuantas sesiones sobre el total => eso es la probabilidad de mod
q' = 'hello'
CQ1 = hello world
CQ2 = hello my
=> I have to look for all sessions which 𝑞′ appears and then look for the next queries of the same session (same user) to see if 𝑞′ has added words, if it does (like 'hello world' as an example) then add it to the dictionary with word as key and a counter as value. Then for each one calculate the probability based on the total number of sessions in QL in which q’ appears



This is the min of every freq, mod, time across all data. Do we need to sum those values or how do they interact with 1?
1 − 𝑀𝑖𝑛{𝐹𝑟𝑒𝑞(𝐶𝑄), 𝑀𝑜𝑑(𝐶𝑄, 𝑞′), 𝑇𝑖𝑚𝑒(𝐶𝑄, 𝑞′)} => el mínimo de esos tres valores



in Mod, the part:  the total number of sessions in QL in which q’ appears. This means 

q = texas

1 texas 10:30 => query
1 texas uni 10:31 => candidate
1 texas university 10:32 => candidate
1 texas university students 10:33 => candidate


1 texas 10:30 => query
1 uni 10:31
1 texas university 10:32 => candidate
1 texas university students 10:33 => candidate
2 texas => query
2 texas ss => candidate
3 texas => query
4 texas instr


QUESTIONS:
𝑇𝑖𝑚𝑒(𝐶𝑄, 𝑞′) is min difference between the time occurrence of q’ and 𝐶𝑄 across sessions in which both q’ and 𝐶𝑄 appear over length of the longest session in QL.
Can we talk more about the Time calculation? How should we calculate it? what about the longest session? 



***************************************************************************

Technical Notes

To start mongo: 
cd  ~/Software/mongodb/bin
./mongod
- ./mongo 	=> mongo client
https://docs.mongodb.com/manual/reference/mongo-shell/
use <db>
To recreate collections:
db.collection_documents.drop()
db.inverted_index.drop()
db.max_freq_doc.drop()

db.createCollection("collection_documents")
db.createCollection("max_freq_doc")
db.createCollection("inverted_index")    

db.getCollection('collection_documents').createIndex({id: 1})
db.getCollection('max_freq_doc').createIndex({doc_id: 1})
db.getCollection('inverted_index').createIndex({term: 1})

db.getCollection('collection_documents').find({}).length()
db.getCollection('inverted_index').find({}).length()
db.getCollection('max_freq_doc').find({}).length()

db.getCollection('collection_documents').find({}).length() // 630000
db.getCollection('collection_documents').find({id: 620001})
db.getCollection('collection_documents').remove({id: {$gt: 620000}})

db.getCollection('max_freq_doc').find({doc_id: 620000})


To start redis:
redis-server /usr/local/etc/redis.conf
redis-cli => redis client
FLUSHALL => to delete all keys from all Redis databases
FLUSHDB => To delete all keys of the selected Redis database only
del myhash => delete my entire mynahs
HGETALL myhash => get all keys and values from myhash
	 HGETALL collection_documents 
	 HGETALL inverted_index
HLEN myhash => returns the size of elements
HGET collection_documents 1000
GET tubul

We're using python3:
python3 controller.py


Execution Logs:
Test: Save in mongo documents and index
1000 => 173.997 seconds (2.90 minutes)
for 1.6 million docs, it will take 80 hours

Test: Save index in memory only
1000 => 10 seconds in multiprocessing. 32 seconds in serie.
2000 => 16 seconds in multiprocessing. 77.31 seconds in serie.
20000 => 166.38 seconds in multiprocessing.  seconds in serie.
for 1.6 million docs, it will take 36.97 hours

Test: Save index in redis only
1000 => 24.65 seconds in multiprocessing. 
2000 => 43.63 seconds in multiprocessing.
20000 => 184.97 seconds in multiprocessing.

Test: Save in mongo the index
1000 => 145.92 seconds


ubuntu commands for mongodb:
sudo systemctl enable mongod
sudo systemctl start mongod 

sudo systemctl stop mongod
sudo systemctl restart mongod 

test on ubuntu:
start time: 2019-10-14 19:29:35.110371


******************
Steps
X = 50 000 docs
1) Loop by batch size of X
2) For every doc read, save it to mongo
3) Calculate the index structures for the batch and loop through them to save a unified index structure for the X docs to save as a pickle file.
4) Loop the folder of the pickle files and iterate them to build a unified index structure, then save that file and iterate through it to save it in mongo.




ñ



